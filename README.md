# AI Interpretability Learning Journey

## Description
Hi there! I'm diving into the world of AI interpretability, and this repo is my digital notebook. 

AI technology has been growing at an exponential speed, especially since the launch of ChatGPT. However, as the initial hype is starting to settle, we're beginning to understand the crucial importance of reliability in AI systems. A key aspect of this reliability is AI interpretability.

My goal is to explore how we can make AI models more transparent and explainable. 

Here's what I plan to do in this repo:
- Share my notes and summaries from papers I read
- Document my experiments and projects related to AI interpretability
- Track my progress and insights as I learn

I believe that by improving our understanding of AI interpretability, we can contribute to the development of more reliable, trustworthy, and effective AI systems.

## Papers Reviewed
- [x] [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html)
- [x] [Towards Monosemanticity: Decomposing Language Models With Dictionary Learning](https://transformer-circuits.pub/2023/monosemantic-features/index.html)
- [x] [Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet](https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html)

### Future Reading List
- [ ] [Extracting Concepts from GPT-4](https://openai.com/index/extracting-concepts-from-gpt-4/)
- [ ] [Interpreting Attention Layer Outputs with Sparse Autoencoders](https://arxiv.org/abs/2406.17759)

## Useful Resources
- [How Interpretable Features in Claude 3 Work | OxenAI](https://youtu.be/bOnpFvN_Fbg?si=_WyzHnrqNBzXQoRo)
- [A Walkthrough of Toy Models of Superposition w/ Jess Smith | Neel Nanda](https://youtu.be/R3nbXgMnVqQ?si=5Al7kIrMww2T-nA_)
- [Towards Monosemanticity: Decomposing Language Models Into Understandable Components | Arize AI](https://youtu.be/hlCxSqWS6Rw?si=GBbrZKJv1a_Ws986)
- [Reading AI's Mind - Mechanistic Interpretability Explained | bycloud](https://youtu.be/Mhp8vpOksWw?si=DoCIzF784vhssI0F)
- [Regularization Part 1: Ridge (L2) Regression | Statquest](https://youtu.be/Q81RR3yKn30?si=E3FfJ9VAIVbr7tuk)
- [Regularization Part 2: Lasso (L1) Regression | Statquest](https://youtu.be/NGf0voTMlcs?si=U3dNbFP4oRnyNLaZ)

## Get in Touch
I'm always eager to learn more! If you have suggestions, questions, or want to discuss AI interpretability, please feel free to:
- Open an issue in this repo
- Reach out to me on [your preferred platform, e.g., Twitter, LinkedIn](https://linktr.ee/ismailko)
- Send me an email at [your email if you're comfortable sharing](mailto:i_konak@hotmail.com)

Your insights and feedback are greatly appreciated!

## Project Ideas
Make a basic NN interpretable. For this task I chosed the MNIST dataset and a simple 3-layer neural network. (contd.)
